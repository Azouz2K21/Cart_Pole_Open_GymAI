{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0425023 ,  0.00437766,  0.01588009,  0.01659871], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:163: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "for dummy in range(100):\n",
    "    environment.render()\n",
    "    environment.step(environment.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01365825,  0.04215448,  0.03110193, -0.03966234], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_buckets = (1, 1, 6, 3)\n",
    "no_actions = environment.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_value_bounds = list(zip(environment.observation_space.low,\n",
    "environment.observation_space.high))\n",
    "state_value_bounds[1] = [-0.5, 0.5]\n",
    "state_value_bounds[3] = [-math.radians(50), math.radians(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_index = len(no_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_value_table = np.zeros(no_buckets + (no_actions,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_explore_rate = 0.01\n",
    "min_learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episodes = 1000\n",
    "max_time_steps = 250\n",
    "streak_to_end = 120\n",
    "solved_time = 199\n",
    "discount = 0.99\n",
    "no_streaks = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state_value, explore_rate):\n",
    "    if random.random() <explore_rate:\n",
    "        action = environment.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_value_table[state_value])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_explore_rate(x):\n",
    "    return max(min_explore_rate, min(1, 1.0 - math.log10((x+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_learning_rate(x):\n",
    "    return max(min_learning_rate, min(0.5, 1.0 - math.log10((x+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bucketize_state_value(state_value):\n",
    "    bucket_indexes = []\n",
    "    for i in range(len(state_value)):\n",
    "        if state_value[i] <= state_value_bounds[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state_value[i] >= state_value_bounds[i][1]:\n",
    "            bucket_index = no_buckets[i] - 1\n",
    "        else:\n",
    "            bound_width = state_value_bounds[i][1] - state_value_bounds[i][0]\n",
    "            offset = (no_buckets[i]-1)*state_value_bounds[i][0]/bound_width\n",
    "            scaling = (no_buckets[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state_value[i] - offset))\n",
    "            bucket_indexes.append(bucket_index)\n",
    "    return tuple(bucket_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode_no in range(max_episodes):\n",
    "    explore_rate = select_explore_rate(episode_no)\n",
    "    learning_rate = select_learning_rate(episode_no)\n",
    "\n",
    "observation = environment.reset()\n",
    "\n",
    "start_state_value = bucketize_state_value(observation)\n",
    "previous_state_value = start_state_value\n",
    " \n",
    "for time_step in range(max_time_steps):\n",
    "    environment.render()\n",
    "    selected_action = select_action(previous_state_value, explore_rate)\n",
    "    observation, reward_gain, completed, _ = environment.step(selected_action)\n",
    "    state_value = bucketize_state_value(observation)\n",
    "    best_q_value = q_value_table[state_value] #np.amax(q_value_table[state_value])\n",
    "    q_value_table[previous_state_value + (selected_action,)] += learning_rate * (reward_gain + discount * (best_q_value) - q_value_table[previous_state_value + (selected_action,)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'episode_no' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mo112\\Desktop\\Aero\\AE8152\\Project 3\\CartPole.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mo112/Desktop/Aero/AE8152/Project%203/CartPole.ipynb#ch0000015?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpisode number : \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m episode_no)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mo112/Desktop/Aero/AE8152/Project%203/CartPole.ipynb#ch0000015?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTime step : \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m time_step)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mo112/Desktop/Aero/AE8152/Project%203/CartPole.ipynb#ch0000015?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSelection action : \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m selected_action)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'episode_no' is not defined"
     ]
    }
   ],
   "source": [
    "print('Episode number : %d' % episode_no)\n",
    "print('Time step : %d' % time_step)\n",
    "print('Selection action : %d' % selected_action)\n",
    "print('Current state : %s' % str(state_value))\n",
    "print('Reward obtained : %f' % reward_gain)\n",
    "print('Best Q value : %f' % best_q_value)\n",
    "print('Learning rate : %f' % learning_rate)\n",
    "print('Explore rate : %f' % explore_rate)\n",
    "print('Streak number : %d' % no_streaks)\n",
    "if completed:\n",
    "    print('Episode %d finished after %f time steps' % (episode_no, time_step))\n",
    "if time_step>= solved_time:\n",
    "    no_streaks += 1\n",
    "else:\n",
    "    no_streaks = 0\n",
    "    break\n",
    " \n",
    "previous_state_value = state_value\n",
    " \n",
    "if no_streaks>streak_to_end:\n",
    "    break   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 0] - Mean survival time over last 100 episodes was 18.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 15.36 ticks.\n",
      "Did not solve after 199 episodes ðŸ˜ž\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, n_episodes=200, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v1')\n",
    "        self.env.theta_threshold_radians = 0.20944\n",
    "        self.env.x_threshold = 2.4\n",
    "        self.env.force_mag = 100\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Dense(24, input_dim=4, activation='tanh'))\n",
    "        self.model.add(Dense(48, activation='tanh'))\n",
    "        self.model.add(Dense(2, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.model.predict(state))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            y_target = self.model.predict(state)\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * np.max(self.model.predict(next_state)[0])\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.model.fit(np.array(x_batch), np.array(y_batch), batch_size=len(x_batch), verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver()\n",
    "    agent.run()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "665bc03ec7fbb21b609141bff26cf1222b7f237ce4f80dba6f9953df3f1f8d3c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
